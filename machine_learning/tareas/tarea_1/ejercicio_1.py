# -*- coding: utf-8 -*-
"""Tarea 2 Optimizaciones

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SopHctkQ5OJSCPnYczOevxvNM2WJTJc-

# **Tarea 2: Descenso de Gradiente**
**Nombre:** Noé Guadalupe Aldana Murillo  
**Semestre:** Primavera 2025
"""

import numpy as np
import matplotlib.pyplot as plt



def gradiente(f, grad_f, x0, learning_rate, max_iter, c1=1e-4, c2=0.9):
    x = x0
    history = [x0]
    for i in range(max_iter):
        grad = grad_f(x)

        while f(x -learning_rate * grad) > f(x) - c1 * learning_rate * grad.dot(grad) or grad_f(x - learning_rate * grad).dot(grad) <  c2 * grad.dot(grad):
            learning_rate *= 0.5
        x = x - learning_rate * grad
        history.append(x)
    return x,history

def visualizar_general(f, grad_f, x0, learning_rate, max_iter):
    x0 = np.array(x0)  # asegurarse de trabajar con un array
    dim = x0.shape[0]
    x_min, history = gradiente(f, grad_f, x0, learning_rate, max_iter)
    history = np.array(history)

    print("Resultado:")
    print(f"Valor mínimo: x = {x_min}, f(x) = {f(x_min)}")

    if dim == 1:
        # Visualización 1D
        x_vals = np.linspace(x_min[0] - 5, x_min[0] + 5, 200)
        y_vals = f(x_vals)
        plt.figure(figsize=(8, 4))
        plt.plot(x_vals, y_vals, label='f(x)')
        # La trayectoria se traza como puntos
        plt.plot(history.flatten(), f(history.flatten()), 'ro-', label='Trayectoria')
        plt.xlabel('x')
        plt.ylabel('f(x)')
        plt.legend()
        plt.title("Visualización 1D")
        plt.show()

    elif dim == 2:
        # Visualización 2D: Contour plot
        x_min_hist, x_max_hist = history[:, 0].min() - 1, history[:, 0].max() + 1
        y_min_hist, y_max_hist = history[:, 1].min() - 1, history[:, 1].max() + 1
        x_vals = np.linspace(x_min_hist, x_max_hist, 100)
        y_vals = np.linspace(y_min_hist, y_max_hist, 100)
        X, Y = np.meshgrid(x_vals, y_vals)
        Z = np.array([[f(np.array([xi, yi])) for xi in x_vals] for yi in y_vals])

        plt.figure(figsize=(8, 6))
        cp = plt.contour(X, Y, Z, levels=30, cmap='viridis')
        plt.clabel(cp, inline=True, fontsize=8)
        plt.plot(history[:, 0], history[:, 1], 'ro-', label='Trayectoria')
        plt.scatter(x_min[0], x_min[1], color='green', s=100, label='Mínimo')
        plt.xlabel('x')
        plt.ylabel('y')
        plt.legend()
        plt.title("Visualización 2D")
        plt.show()

    elif dim == 3:
        # Visualización 3D: Usamos un gráfico 3D para la trayectoria.
        fig = plt.figure(figsize=(10, 6))
        ax = fig.add_subplot(111, projection='3d')

        # Para graficar la "superficie" es un poco más complejo ya que f: R^3 -> R.
        # Aquí haremos una proyección usando las dos primeras dimensiones y fijamos la tercera.
        x_min_hist, x_max_hist = history[:, 0].min() - 1, history[:, 0].max() + 1
        y_min_hist, y_max_hist = history[:, 1].min() - 1, history[:, 1].max() + 1
        x_vals = np.linspace(x_min_hist, x_max_hist, 50)
        y_vals = np.linspace(y_min_hist, y_max_hist, 50)
        X, Y = np.meshgrid(x_vals, y_vals)
        fixed_z = x_min[2]  # o bien la media de la tercera coordenada

        Z = np.array([[f(np.array([xi, yi, fixed_z])) for xi in x_vals] for yi in y_vals])

        ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)

        # Calcular f para cada punto de la trayectoria
        traj_z = np.array([f(point) for point in history])
        ax.plot(history[:, 0], history[:, 1], traj_z, 'ro-', label='Trayectoria')
        ax.scatter(x_min[0], x_min[1], f(x_min), color='green', s=100, label='Mínimo')

        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_zlabel('f(x)')
        ax.set_title("Visualización 3D")
        ax.legend()
        plt.show()

    else:
        print("Visualización no soportada para dimensiones mayores a 3.")

"""

## **Ejercicio 1: Minimización de una función cuadrática**  
Minimiza la función $f(x) = (x - 3)^2$ usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = 0.0$  
- **Tasa de aprendizaje:** 0.1  
- **Iteraciones:** 50  


"""

# ======== EJERCICIO 1 =========
# Parametros iniciales
x0=np.array([0.0])
learning_rate = 0.1
max_iter = 50


# Definir la función y su gradiente
f = lambda x: (x - 3) ** 2
func_gradiente = lambda x: 2 * (x - 3)


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""## **Ejercicio 2: Minimización de una función multivariable**  
Minimiza la función $f(x, y) = (x - 1)^2 + (y - 2)^2$ usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (0,0)$  
- **Tasa de aprendizaje:** 0.1  
- **Iteraciones:** 100  
"""

# ======== EJERCICIO 2 =========
# Parametros iniciales
x0=np.array([0.0,0.0])
learning_rate = 0.1
max_iter = 100


# Definir la función y su gradiente
f = lambda x: (x[0] - 1) ** 2 + (x[1] - 2) ** 2
func_gradiente = lambda x: np.array([
    2 * (x[0] - 1),
    2 * (x[1] - 2)
])


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""
## **Ejercicio 3: Minimización de la función de Himmelblau**  
Minimiza la función de Himmelblau usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (0,0)$  
- **Tasa de aprendizaje:** 0.01  
- **Iteraciones:** 10,000  

"""

# ======== EJERCICIO 3 =========
# Parametros iniciales
x0=np.array([0.0,0.0])
learning_rate = 0.01
max_iter = 10000

# Definir la función y su gradiente
f = lambda x: (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2
func_gradiente = lambda x: np.array([
    4 * x[0] * (x[0]**2 + x[1] - 11) + 2 * (x[0] + x[1]**2 - 7),
    2 * (x[0]**2 + x[1] - 11) + 4 * x[1] * (x[0] + x[1]**2 - 7)
])

# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""## **Ejercicio 4: Minimización de la función de Beale**  
Minimiza la función de Beale usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (1,1)$  
- **Tasa de aprendizaje:** 0.001  
- **Iteraciones:** 50,000   
"""

# ======== EJERCICIO 4 =========
# Parametros iniciales
x0=np.array([1,1])
learning_rate = 0.001
max_iter = 50000


# Definir la función y su gradiente
f = lambda x: (1.5 - x[0] + x[0] * x[1]) ** 2 + (2.25 - x[0] + x[0] * x[1]**2) ** 2 + (2.625 - x[0] + x[0] * x[1]**3) ** 2
func_gradiente = lambda x: np.array([
    2 * (1.5 - x[0] + x[0] * x[1]) * (-1 + x[1]) +
    2 * (2.25 - x[0] + x[0] * x[1]**2) * (-1 + x[1]**2) +
    2 * (2.625 - x[0] + x[0] * x[1]**3) * (-1 + x[1]**3),

    2 * (1.5 - x[0] + x[0] * x[1]) * x[0] +
    4 * (2.25 - x[0] + x[0] * x[1]**2) * x[0] * x[1] +
    6 * (2.625 - x[0] + x[0] * x[1]**3) * x[0] * x[1]**2
])


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""## **Ejercicio 5: Minimización de la función de Booth**  
Minimiza la función de Booth usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (0,0)$  
- **Tasa de aprendizaje:** 0.01  
- **Iteraciones:** 1,000  


"""

# ======== EJERCICIO 5 =========
# Parametros iniciales
x0=np.array([1.0,3.0])
learning_rate = 0.01
max_iter = 1000


# Definir la función y su gradiente
f = lambda x: (x[0] + 2*x[1] - 7) ** 2 + (2*x[0] + x[1] - 5) ** 2
func_gradiente = lambda x: np.array([
    2 * (x[0] + 2*x[1] - 7) + 4 * (2*x[0] + x[1] - 5),
    4 * (x[0] + 2*x[1] - 7) + 2 * (2*x[0] + x[1] - 5)
])


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""## **Ejercicio 6: Minimización de la función de Matyas**  
Minimiza la función de Matyas usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (0,0)$  
- **Tasa de aprendizaje:** 0.1  
- **Iteraciones:** 100  


"""

# ======== EJERCICIO 6 =========
# Parametros iniciales
x0=np.array([0.5,0.5])
learning_rate = 0.1
max_iter = 100


# Definir la función y su gradiente
f = lambda x: (x[0] + 2*x[1] - 7) ** 2 + (2*x[0] + x[1] - 5) ** 2
func_gradiente = lambda x: np.array([
    2 * (x[0] + 2*x[1] - 7) + 4 * (2*x[0] + x[1] - 5),
    4 * (x[0] + 2*x[1] - 7) + 2 * (2*x[0] + x[1] - 5)
])


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

"""## **Ejercicio 7: Minimización de la función de Levi**  
Minimiza la función de Levi usando el algoritmo de descenso de gradiente.  
- **Punto inicial:** $x_0 = (1,1)$  
- **Tasa de aprendizaje:** 0.01  
- **Iteraciones:** 1,000
"""

# ======== EJERCICIO 7 =========
# Parametros iniciales
x0=np.array([1.0,1.0])
learning_rate = 0.01
max_iter = 1000


# Definir la función y su gradiente
f = lambda x: np.sin(3*np.pi*x[0])**2 + (x[0]-1)**2*(1+np.sin(3*np.pi*x[1])**2) + (x[1]-1)**2*(1+np.sin(2*np.pi*x[1])**2)
func_gradiente = lambda x: np.array([
    6*np.pi*np.cos(3*np.pi*x[0])*np.sin(3*np.pi*x[0]) + 2*(x[0]-1)*(1+np.sin(3*np.pi*x[1])**2),
    6*np.pi*(x[0]-1)**2*np.cos(3*np.pi*x[1])*np.sin(3*np.pi*x[1]) + 2*(x[1]-1)*(1+np.sin(2*np.pi*x[1])**2) + 4*np.pi*(x[1]-1)**2*np.cos(2*np.pi*x[1])*np.sin(2*np.pi*x[1])
])


# Visualizar
visualizar_general(f, func_gradiente, x0, learning_rate, max_iter)

